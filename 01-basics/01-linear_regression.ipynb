{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lento234/ml-tutorials/blob/main/01-basics/01-linear_regression.ipynb)\n",
    "\n",
    "**References**:\n",
    "- https://pytorch.org/\n",
    "- https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/linear_regression/main.py\n",
    "- https://en.wikipedia.org/wiki/Linear_regression\n",
    "- https://en.wikipedia.org/wiki/Backpropagation\n",
    "- https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "\n",
    "\n",
    "\n",
    "**Linear regression**\n",
    "\n",
    "Given a data set $\\{y_i, x_i\\}_{i=1}^n$ of $n$ statistical units, we have the following relationship:\n",
    "\n",
    "$$ y_i = \\beta_1 x_i + \\beta_0 + \\varepsilon_i, \\qquad i=1,\\ldots, n,$$\n",
    "\n",
    "\n",
    "where:\n",
    "- $y_i$ is observed value (dependent variable\n",
    "- $x_i$ is the independent variable\n",
    "- $\\beta_1$ and $\\beta_0$ are the trainable parameters\n",
    "- $\\varepsilon_i$ is the measurement error\n",
    "\n",
    "**Table of Content**\n",
    "1. [Setup environment](#setup)\n",
    "2. [Define the model, loss function and optimizer](#model)\n",
    "3. [Train the model](#train)\n",
    "4. [Predict and evaluate the model](#evaluate)\n",
    "5. [Save the trained model](#save)\n",
    "6. [Load pretrained model](#load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. Setup environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages / modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.style.use('seaborn-poster')\n",
    "mpl.rcParams['mathtext.fontset'] = 'cm'\n",
    "mpl.rcParams['figure.figsize'] = 5 * np.array([1.618033988749895, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducability\n",
    "seed = 234\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "beta_1 = 2.0\n",
    "beta_0 = 0.3\n",
    "eps_std = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.random.rand(n,1)-0.5\n",
    "eps = np.random.normal(scale=eps_std, size=(n,1))\n",
    "y_train = x_train*beta_1 + beta_0 + eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_train, y_train, 'k.', label='observations')\n",
    "ax.plot(x_train, x_train*beta_1 + beta_0, 'tab:gray',\n",
    "        label=r'analytical ($\\beta_1={:.2f}$, $\\beta_0={:.2f}$)'.format(beta_1, beta_0))\n",
    "ax.set(xlabel='$x$', ylabel='$y$', xlim=(-0.52,0.52), ylim=(-1.0, 1.5));\n",
    "# ax.legend();\n",
    "fig.legend(loc='right', bbox_to_anchor=(1.5, 0.6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "## 2. Define the model, loss function and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1 # number of nodes (1 weight + 1 bias)\n",
    "num_epochs = 150 # i.e. number of iterations\n",
    "learning_rate = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "$$ \\mathcal{F}: x \\rightarrow y $$\n",
    "\n",
    "**Linear 1-D model**:\n",
    "\n",
    "$$ \\mathcal{F}(x) = \\hat{y} = x a + b$$\n",
    "\n",
    "where:\n",
    "- $y$ is the ground truth\n",
    "- $\\hat{y}$ is the predicted output\n",
    "- $a$ is the learnable weight, i.e. $a \\equiv \\beta_1$\n",
    "- $b$ is the learnable bias, i.e. $b \\equiv \\beta_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression model\n",
    "model = nn.Linear(in_features=n_features, out_features=n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "**$L^1$-norm:**\n",
    "\n",
    "$$ \\mathcal{L}(X, Y) = \\textrm{mean} \\left( \\{l_1,\\dots,l_N\\}^\\top \\right), \\quad l_n = \\left| \\hat{y}_n - y_n \\right| $$\n",
    "where:\n",
    "- $X \\in \\mathbb{R}^N$ is the input matrix\n",
    "- $Y \\in \\mathbb{R}^N$ is the ground truth matrix\n",
    "- $N$ is the batch size (for now, batch size = number of examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.L1Loss() # L^1-norm, aka. mean absolute error loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "**Stochastic gradient descent**\n",
    "\n",
    "$$ \\theta_{n+1} = \\theta_n - \\eta \\nabla \\mathcal{L}(\\theta_n) $$\n",
    "\n",
    "where:\n",
    "- $\\theta$ are the trainable parameters\n",
    "- $\\eta$ is the learning rate (i.e. step size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # Stochastic gradient-descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "## 3. Train the model\n",
    "\n",
    "$$ \\mathcal{F}^* = \\arg \\min_{\\mathcal{F}}\\ \\mathcal{L}(X, Y)$$\n",
    "\n",
    "**Algorithm:**\n",
    "1. Convert data to torch tensor: `torch.from_numpy(...)`\n",
    "2. Forward pass: `y_pred = model(x)`\n",
    "3. Calculate loss: `loss = criterion(y_pred, y)`\n",
    "4. Compute gradients: `loss.backward()`\n",
    "5. Update weights: `optimizer.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = dict(loss=[], parameters=[])\n",
    "for epoch in range(num_epochs):\n",
    "    # 1. Convert numpy arrays to torch tensors\n",
    "    x = torch.from_numpy(x_train.astype('float32'))\n",
    "    y = torch.from_numpy(y_train.astype('float32'))\n",
    "\n",
    "    # 2. Forward pass\n",
    "    y_pred = model(x) # prediction step\n",
    "    \n",
    "    # 3. Calculate loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    # 4. Backward propagation \n",
    "    optimizer.zero_grad() # reset gradients to zero\n",
    "    loss.backward() # backprop: calculate gradients w.r.t to loss\n",
    "    \n",
    "    # 5. Update weights\n",
    "    optimizer.step() # update gradient\n",
    "    \n",
    "    # 6. Log\n",
    "    history['loss'].append(loss.item())\n",
    "    history['parameters'].append([param.detach().item() for param in model.parameters()])\n",
    "    if (epoch % 10) == 0 or epoch==(num_epochs-1):\n",
    "        print ('[Epoch {}]: loss = {:.4f}'.format(epoch, loss.item()))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(history['loss'], '.-')\n",
    "ax.set(xlabel='epoch', ylabel='MSE loss',\n",
    "       title='Training loss history');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluate\"></a>\n",
    "## 4. Predict and evaluate the model\n",
    "\n",
    "### Training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "predict = model(torch.from_numpy(x_train.astype('float32'))).detach().numpy()\n",
    "\n",
    "# Plot the graph\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_train, y_train, 'k.', label='observations')\n",
    "ax.plot(x_train, x_train*beta_1 + beta_0, 'tab:gray', lw=5,\n",
    "        label=r'analytical: ($\\beta_1={:.2f}$, $\\beta_0={:.2f}$)'.format(beta_1, beta_0))\n",
    "ax.plot(x_train, x_train*history['parameters'][0][0] + history['parameters'][0][1], 'tab:blue',\n",
    "        label=r'prediction [epoch 0]: ($\\beta_1={:.2f}$, $\\beta_0={:.2f}$)'.format(*history['parameters'][0]))\n",
    "ax.plot(x_train, predict, 'tab:red',\n",
    "        label=r'prediction [trained]: ($\\beta_1={:.2f}$, $\\beta_0={:.2f}$)'.format(*history['parameters'][-1]))\n",
    "ax.set(xlabel='$x$', ylabel='$y$',\n",
    "       title='Training accuracy',\n",
    "       xlim=(-0.52,0.52), ylim=(-1.0, 1.5))\n",
    "\n",
    "fig.legend(loc='right', bbox_to_anchor=(1.6, 0.6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization over the loss landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L^1 norm: numpy version\n",
    "l1_norm = lambda y_hat, y: np.abs(y_hat - y).mean(axis=0)\n",
    "\n",
    "# Hypothesis space\n",
    "beta_1_h, beta_0_h = np.meshgrid(np.linspace(0, 4),\n",
    "                                 np.linspace(-0.2, 0.8))\n",
    "obj_surf = l1_norm(x_train[...,None]*beta_1_h[None] + beta_0_h[None],\n",
    "                   y_train[...,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.contourf(beta_1_h, beta_0_h, obj_surf)\n",
    "ax.plot(*history['parameters'][0], 'c.', ms=15, label='start', zorder=10)\n",
    "ax.plot(beta_1, beta_0, 'r.', ms=20, label='optima', zorder=10)\n",
    "ax.plot(np.array(history['parameters'])[:,0], np.array(history['parameters'])[:,1],\n",
    "        '.-', c='k', label='path')\n",
    "ax.set(xlabel=r'$\\beta_1$ (slope)', ylabel=r'$\\beta_0$ (offset)')\n",
    "fig.colorbar(im, ax=ax)\n",
    "fig.legend(loc='right', bbox_to_anchor=(1.15, 0.7));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save\"></a>\n",
    "## 5. Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "## 6. Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('model.ckpt')\n",
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
