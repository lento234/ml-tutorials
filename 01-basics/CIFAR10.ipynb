{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10: Training a classifer\n",
    "\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lento234/ml-tutorials/blob/main/01-basics/CIFAR10.ipynb)\n",
    "\n",
    "**References**:\n",
    "- https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Runtime setup: GPU accelerator at Google colab:**\n",
    "\n",
    "1. On the main menu, click **Runtime** and select **Change runtime type**. \n",
    "2. Select **GPU** as the hardware accelerator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![steps](../images/steps.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of content**\n",
    "\n",
    "1. [Load and pre-process the dataset](#load)\n",
    "2. [Define the CNN model](#define)\n",
    "3. [Define the loss function and optimizer](#loss)\n",
    "4. [Train the model on **training** dataset](#train)\n",
    "5. [Test/Validate the model on **test** dataset](#validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CIFAR10 Dataset**\n",
    "\n",
    "The dataset consists of `3x32x32` images of 10 difference classes:\n",
    "\n",
    "    airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck.\n",
    "\n",
    "![cifar10](../images/cifar10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing platform: CPU vs. GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Computing with <<{device}>>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_workers = 4\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load'></a>\n",
    "## 1. Load and pre-process data\n",
    "\n",
    "- Define preprocessing algorithm\n",
    "- Load training and test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define preprocessing algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # convert data to pytorch tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # normalize dataset for each channel\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download train and test dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                 download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, \n",
    "                                 download=True, transform=transform)\n",
    "\n",
    "# Dataset sampler (shuffle, distributed loading)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                           shuffle=True, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, \n",
    "                                          shuffle=False, num_workers=num_workers)\n",
    "\n",
    "print(f\"num. examples: train = {len(train_dataset)}, test = {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array(['plane', 'car', 'bird', 'cat', 'deer',\n",
    "                    'dog', 'frog', 'horse', 'ship', 'truck'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(images, labels):    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    for i in range(16):\n",
    "        plt.subplot(4,4,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        img = images[i] / 2 + 0.5 # unnormalize\n",
    "        plt.imshow(np.transpose(img.numpy(), (1, 2, 0)), cmap=plt.cm.binary)\n",
    "        plt.xlabel(labels[i])\n",
    "    plt.show()\n",
    "    \n",
    "# get some random training images\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# show images\n",
    "imshow(images, classes[labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=define></a>\n",
    "## 2. Define the CNN model\n",
    "\n",
    "![network_architecture](../images/network_architecture.png)\n",
    "\n",
    "**Architecture:**\n",
    "\n",
    "- Input: An image of `n_channels=3`.\n",
    "- Two layer stacks of 2D convolutional layers (`Conv2d` with `kernel_size=5`) with rectified linear activation (`ReLU`) followed by a  2D max pooling (`MaxPool2D` with `kernel_size=2` and `stride=2`)\n",
    "- Three layer stacks of Fully-connected layers (`Linear`) with ReLU activaton.\n",
    "- Output: 10-dimensional vector defining the activation of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Define network\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(3, 6, kernel_size=5),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 =  nn.Sequential(nn.Conv2d(6, 16, kernel_size=5),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential(nn.Flatten(),\n",
    "                                    nn.Linear(16 * 5 * 5, 120),\n",
    "                                    nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(nn.Linear(120, 84),\n",
    "                                    nn.ReLU())\n",
    "        self.layer5 = nn.Linear(84, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=loss></a>\n",
    "## 3. Define the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "print(\"loss:\", criterion)\n",
    "print(\"optimizer:\", optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=train></a>\n",
    "## 4. Train the model on **training** dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (x_train, y_train) in tqdm(enumerate(train_loader), \n",
    "                                              desc=f\"[Epoch {epoch}]\",\n",
    "                                              total=len(train_loader)):\n",
    "        # Send batch to device\n",
    "        x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward-pass\n",
    "        y_pred = model(x_train)\n",
    "        \n",
    "        # backward propogation\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        \n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    loss_history.append(running_loss / len(train_loader))\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(loss_history, '.-')\n",
    "ax.set(xlabel='epoch', ylabel='loss',\n",
    "       title='loss history');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"validate\"></a>\n",
    "## 5. Test/Validate the model on **test** dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Predict using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(test_loader))\n",
    "\n",
    "# predict\n",
    "prediction = model(images.to(device))\n",
    "prediction = prediction.cpu() # gpu -> cpu\n",
    "predicted_labels = torch.argmax(prediction, 1).detach()\n",
    "\n",
    "# print images\n",
    "imshow(images, [\"{} ({}) {}\".format(classes[pred], classes[gt], \"✓\" if pred == gt else \"x\")\n",
    "                for pred, gt in zip(predicted_labels, labels)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Accuracy of the model on **test** dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, total=len(test_loader)):\n",
    "        prediction = model(images.to(device)).cpu()\n",
    "        predicted_labels = torch.argmax(prediction, 1)\n",
    "        total += predicted_labels.size(0)\n",
    "        correct += (predicted_labels == labels).sum().item()\n",
    "\n",
    "print('Accuracy on {} test images: {}%'.format(\n",
    "    len(test_loader)*batch_size,\n",
    "     100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Accuracy of the model per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = np.zeros(len(classes))\n",
    "class_total = np.zeros(len(classes))\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, total=len(test_loader)):\n",
    "        prediction = model(images.to(device)).cpu()\n",
    "        predicted_labels = torch.argmax(prediction, 1)\n",
    "        c = (predicted_labels == labels).squeeze()\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "sorted_idx = np.argsort(class_correct/class_total)[::-1]\n",
    "for i in sorted_idx:\n",
    "    print('Accuracy of {:5s}: {}%'.format(\n",
    "           classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
